{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5a95c89-d332-41b2-b3a6-9af23c67c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, random_split\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torchvision import datasets, transforms\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import RandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bf3c664-f2c1-464b-98b8-f4cba7e8f5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation functions\n",
    "def rotate_point_cloud(points):\n",
    "    \"\"\"Rotate the point cloud around the z-axis.\"\"\"\n",
    "    angle = np.random.uniform(0, 2 * np.pi)\n",
    "    cos_val = np.cos(angle)\n",
    "    sin_val = np.sin(angle)\n",
    "    rotation_matrix = np.array([\n",
    "        [cos_val, -sin_val, 0],\n",
    "        [sin_val, cos_val, 0],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    rotated_points = np.dot(points, rotation_matrix.T)\n",
    "    return rotated_points\n",
    "\n",
    "def translate_point_cloud(points):\n",
    "    \"\"\"Translate the point cloud by a small random vector.\"\"\"\n",
    "    translation_vector = np.random.uniform(-0.2, 0.2, size=(1, 3))\n",
    "    translated_points = points + translation_vector\n",
    "    return translated_points\n",
    "\n",
    "def scale_point_cloud(points):\n",
    "    \"\"\"Scale the point cloud by a random factor.\"\"\"\n",
    "    scale_factor = np.random.uniform(0.8, 1.2)\n",
    "    scaled_points = points * scale_factor\n",
    "    return scaled_points\n",
    "\n",
    "def jitter_point_cloud(points):\n",
    "    \"\"\"Add Gaussian noise to the point cloud.\"\"\"\n",
    "    noise = np.random.normal(0, 0.01, points.shape)\n",
    "    jittered_points = points + noise\n",
    "    return jittered_points\n",
    "\n",
    "def augment_point_cloud(points):\n",
    "    points = rotate_point_cloud(points)\n",
    "    points = translate_point_cloud(points)\n",
    "    points = scale_point_cloud(points)\n",
    "    points = jitter_point_cloud(points)\n",
    "    return points\n",
    "\n",
    "def augment_point_cloud(features):\n",
    "    \"\"\"Augment the point cloud with random rotations and/or noise.\"\"\"\n",
    "    # Extract the spatial coordinates (x, y, z)\n",
    "    points = features[:, :3]  # Only the x, y, z columns\n",
    "    \n",
    "    # Check and remove the header row if present\n",
    "    if isinstance(points[0, 0], str):  # Check if the first element is a string (indicating header)\n",
    "        points = points[1:, :]  # Remove the first row (header)\n",
    "        features = features[1:, :]  # Remove the header row from the whole features array as well\n",
    "    \n",
    "    # Convert points to float type explicitly\n",
    "    points = points.astype(np.float32)\n",
    "    \n",
    "    # Print the shape of points to debug\n",
    "    print(\"Shape of points:\", points.shape)\n",
    "    \n",
    "    # Print the first few points to check the data\n",
    "    print(\"First few points:\", points[:5])\n",
    "    \n",
    "    # Generate a random rotation matrix (for rotating around the z-axis)\n",
    "    angle = np.random.uniform(-np.pi, np.pi)\n",
    "    rotation_matrix = np.array([\n",
    "        [np.cos(angle), -np.sin(angle), 0],\n",
    "        [np.sin(angle), np.cos(angle), 0],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    \n",
    "    # Print the rotation matrix\n",
    "    print(\"Rotation matrix:\\n\", rotation_matrix)\n",
    "    \n",
    "    # Apply the rotation to the points (only x, y, z)\n",
    "    try:\n",
    "        rotated_points = np.dot(points, rotation_matrix.T)  # Rotate points\n",
    "    except Exception as e:\n",
    "        print(\"Error during matrix multiplication:\", e)\n",
    "        return features\n",
    "    \n",
    "    # Replace the original points with the rotated points\n",
    "    features[:, :3] = rotated_points\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f509a551-6381-466d-b0fe-4d6b43d31625",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def normalize_point_cloud(points):\n",
    "    \"\"\"Normalize the point cloud.\"\"\"\n",
    "    centroid = np.mean(points, axis=0)\n",
    "    points -= centroid\n",
    "    max_dist = np.max(np.sqrt(np.sum(points**2, axis=1)))\n",
    "    points /= max_dist\n",
    "    return points\n",
    "\n",
    "class PointCloudDataset(Dataset):\n",
    "    def __init__(self, data_dir, augment=False):\n",
    "        self.data_dir = data_dir\n",
    "        self.files = [f for f in os.listdir(data_dir) if f.endswith('.npy')]\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.data_dir, self.files[idx])\n",
    "        \n",
    "        data = pd.read_csv(file_path, header=None).values  # Load as NumPy array\n",
    "        points = data[:, :3]  # x, y, z\n",
    "        normals = data[:, 3:6]  # nx, ny, nz\n",
    "        curvature_density = data[:, 6:8]  # curvature, density\n",
    "        features = np.hstack((points, normals, curvature_density))  # (x, y, z, nx, ny, nz, curvature, density)\n",
    "        labels = curvature_density  # If you're doing regression, these can be your labels\n",
    "        \n",
    "        if self.augment:\n",
    "            features = augment_point_cloud(features)\n",
    "        points = normalize_point_cloud(points)\n",
    "        features = torch.FloatTensor(features)\n",
    "        labels = torch.FloatTensor(labels)  # If you're working with regression, keep labels as float\n",
    "        \n",
    "        return features, labels\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    " \n",
    "    max_points = min(1024, max(item[0].size(0) for item in batch))  # Limit to 1024 points\n",
    "\n",
    "    padded_points = []\n",
    "    padded_labels = []\n",
    "\n",
    "    for points, labels in batch:\n",
    "        \n",
    "        if points.size(0) > max_points:\n",
    "            points = points[:max_points]\n",
    "            labels = labels[:max_points]\n",
    "        \n",
    "        \n",
    "        pad_size = max_points - points.size(0)\n",
    "        \n",
    "   \n",
    "        padded = torch.nn.functional.pad(points, (0, 0, 0, pad_size)) \n",
    "        padded_points.append(padded)\n",
    "        \n",
    "\n",
    "        padded_label = torch.cat((labels, torch.zeros(pad_size)))\n",
    "        padded_labels.append(padded_label)\n",
    "\n",
    "\n",
    "    return torch.stack(padded_points), torch.stack(padded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbd17182-25e4-4de8-8b39-c5c99784a904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PointNetPlusPlus(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PointNetPlusPlus, self).__init__()\n",
    "\n",
    "        # First MLP block with dropout\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Linear(3, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(512)  \n",
    "\n",
    "       \n",
    "        self.residual1 = nn.Linear(3, 512) \n",
    "\n",
    "       \n",
    "        self.abstraction = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)  \n",
    "        )\n",
    "\n",
    "       \n",
    "        self.residual2 = nn.Linear(128, 1) \n",
    "\n",
    "        self.mlp3 = nn.Sequential(\n",
    "            nn.Linear(1, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1) \n",
    "        )\n",
    "        \n",
    "       \n",
    "        self.residual3 = nn.Linear(1, 1)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the first MLP\n",
    "        identity1 = self.residual1(x)\n",
    "        x = self.mlp1(x)\n",
    "        x = x + identity1\n",
    "\n",
    "        # Apply the second MLP with residual connection\n",
    "        x = self.abstraction(x)\n",
    "        identity2 = self.residual2(x)\n",
    "        x = self.mlp2(x)\n",
    "        x = x + identity2\n",
    "\n",
    "        # Apply the third MLP with residual connection\n",
    "        identity3 = self.residual3(x)\n",
    "        x = self.mlp3(x)\n",
    "        x = x + identity3\n",
    "\n",
    "        # Final output\n",
    "        return x.squeeze(-1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d9e9fcf-8555-4300-b21c-20bdc7863bbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: '/home/jovyan/RnD/dataset_npy.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Dataset and DataLoader\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mPointCloudDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn)\n\u001b[1;32m     27\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m PointCloudDataset(data_dir\u001b[38;5;241m=\u001b[39mdata_dir, augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m, in \u001b[0;36mPointCloudDataset.__init__\u001b[0;34m(self, data_dir, augment)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_dir, augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir \u001b[38;5;241m=\u001b[39m data_dir\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maugment \u001b[38;5;241m=\u001b[39m augment\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/home/jovyan/RnD/dataset_npy.zip'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_dir = '/home/jovyan/RnD/dataset_npy.zip' #provide input directory of the filtered data\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 16\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "step_size = 20  \n",
    "gamma = 0.7  \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dataset = PointCloudDataset(data_dir=data_dir, augment=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "val_dataset = PointCloudDataset(data_dir=data_dir, augment=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Model\n",
    "model = PointNetPlusPlus().to(device)\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "\n",
    "epoch_durations = []\n",
    "train_losses = []\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Training Loop\n",
    "best_val_loss = float('inf')\n",
    "def train_one_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for points, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        points, labels = points.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(points)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs.squeeze(), labels.float())  # Squeeze for binary labels\n",
    "        loss.backward()  # Backpropagate\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    return running_loss / len(train_loader)\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for points, labels in tqdm(val_loader, desc=\"Validating\"):\n",
    "            points, labels = points.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(points)\n",
    "            loss = criterion(outputs.squeeze(), labels.float())  # Squeeze for binary labels\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            predictions = (outputs.squeeze() > 0.5).long()  # Assuming binary classification (0 or 1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = correct / total\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "    \n",
    "    \n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_accuracy = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    \n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "    epoch_durations.append(epoch_duration)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "   \n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "    \n",
    "    # Save the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'edgesegment_pointnet_model.pth')\n",
    "        print('Saved the best model!')\n",
    "\n",
    "print('Training finished!')\n",
    "\n",
    "\n",
    "elapsed_times = [sum(epoch_durations[:i+1]) for i in range(num_epochs)] \n",
    "df = pd.DataFrame({\n",
    "    'Elapsed Time (s)': elapsed_times,\n",
    "    'Train Loss': train_losses\n",
    "})\n",
    "df.to_csv('training_time_vs_loss.csv', index=False)\n",
    "\n",
    "# # Plot Training Loss vs Elapsed Time\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(elapsed_times, train_losses, label='Training Loss')\n",
    "# plt.xlabel('Elapsed Time (s)')\n",
    "# plt.ylabel('Training Loss')\n",
    "# plt.title('Training Loss vs Elapsed Time')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.savefig('training_loss_vs_time.png')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeb9d0fd-89ac-4936-831e-1c4a4a01e621",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medgesegment_pointnet_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load('edgesegment_pointnet_model.pth', map_location=device))\n",
    "model.to(device)\n",
    "model.eval()  \n",
    "\n",
    "# Load the test data\n",
    "test_file = '/home/jovyan/dlrv_project_ws/test_pico_2024-07-04_215824.npy' #provide appropriate test data location\n",
    "test_data = np.load(test_file)\n",
    "test_points = torch.tensor(test_data[:, :3], dtype=torch.float32) \n",
    "test_labels = torch.tensor(test_data[:, 3], dtype=torch.float32) \n",
    "\n",
    "# Prepare test data loader\n",
    "test_dataset = torch.utils.data.TensorDataset(test_points, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Testing function\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for points, labels in test_loader:\n",
    "            points, labels = points.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(points)\n",
    "            outputs = outputs.squeeze(-1)  # Shape: [batch_size, num_points]\n",
    "            \n",
    "            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid and round to get [0, 1]\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.numel()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Test the model\n",
    "test_accuracy = test_model(model, test_loader, device)\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
    "\n",
    "\n",
    "# dlrv_project_ws/test_pico_2024-07-04_215824.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c362ad6-5700-4d30-880f-a9dbdfdc134f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/jovyan/dlrv_project_ws/test_Schmersal_AM-T100_2024-07-04_215814.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m test_file = \u001b[33m'\u001b[39m\u001b[33m/home/jovyan/dlrv_project_ws/test_Schmersal_AM-T100_2024-07-04_215814.npy\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Load the test point cloud\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m data = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m points = data[:, :\u001b[32m3\u001b[39m] \n\u001b[32m     44\u001b[39m points = normalize_point_cloud(points)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/opt/conda/envs/itshriks/lib/python3.11/site-packages/numpy/lib/_npyio_impl.py:451\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[39m\n\u001b[32m    449\u001b[39m     own_fid = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m     fid = stack.enter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    452\u001b[39m     own_fid = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    454\u001b[39m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/jovyan/dlrv_project_ws/test_Schmersal_AM-T100_2024-07-04_215814.npy'"
     ]
    }
   ],
   "source": [
    "# Function to plot the point cloud with predicted edge and non-edge points\n",
    "def plot_point_cloud_with_predictions(points, predictions):\n",
    "    \n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "\n",
    "    if isinstance(predictions, torch.Tensor):\n",
    "        predictions = predictions.cpu().numpy()\n",
    "\n",
    "\n",
    "    if predictions.ndim == 1:\n",
    "        predictions = predictions.reshape(-1)\n",
    "\n",
    "\n",
    "    edges = points[predictions == 1]\n",
    "    non_edges = points[predictions == 0]\n",
    "\n",
    "\n",
    "    ax.scatter(edges[:, 0], edges[:, 1], edges[:, 2], c='r', label='Edge', s=1)\n",
    "\n",
    "\n",
    "    ax.scatter(non_edges[:, 0], non_edges[:, 1], non_edges[:, 2], c='b', label='Non-Edge', s=1)\n",
    "\n",
    "\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    plt.legend()\n",
    "    plt.title(\"Detected Edges (Red) and Non-Edges (Blue)\")\n",
    "    \n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# provide Test point cloud file path\n",
    "test_file = '/home/jovyan/dlrv_project_ws/test_Schmersal_AM-T100_2024-07-04_215814.npy'\n",
    "\n",
    "# Load the test point cloud\n",
    "data = np.load(test_file)\n",
    "points = data[:, :3] \n",
    "\n",
    "\n",
    "points = normalize_point_cloud(points)\n",
    "\n",
    "\n",
    "points_tensor = torch.FloatTensor(points).unsqueeze(0).to(device) \n",
    "\n",
    "model = PointNetPlusPlus().to(device)\n",
    "model.load_state_dict(torch.load('edgesegment_pointnet_model.pth')) #load the model using correct path\n",
    "with torch.no_grad():\n",
    "    outputs = model(points_tensor)\n",
    "    print(f\"Model output shape: {outputs.shape}\")\n",
    "    \n",
    "\n",
    "    if outputs.ndimension() == 3:  \n",
    "        outputs = outputs.squeeze(-1)  \n",
    "\n",
    "    predictions = torch.round(torch.sigmoid(outputs)).squeeze()  # [num_points]\n",
    "\n",
    "\n",
    "if predictions.ndimension() == 1:\n",
    "    predictions = predictions.cpu().numpy()\n",
    "else:\n",
    "    predictions = predictions.squeeze().cpu().numpy()\n",
    "\n",
    "\n",
    "# Plot the point cloud with predicted edges and non-edges\n",
    "plot_point_cloud_with_predictions(points, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95fadb98-86d3-4a09-9cb1-b54612349b08",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/jovyan/dlrv_project_ws/training_time_vs_loss.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m file_path= \u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m/home/jovyan/dlrv_project_ws/training_time_vs_loss.csv\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;66;03m#provide the correct filepath for the same\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Plot training loss vs time\u001b[39;00m\n\u001b[32m      5\u001b[39m plt.figure(figsize=(\u001b[32m8\u001b[39m, \u001b[32m6\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/opt/conda/envs/itshriks/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/opt/conda/envs/itshriks/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/opt/conda/envs/itshriks/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/opt/conda/envs/itshriks/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/opt/conda/envs/itshriks/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/jovyan/dlrv_project_ws/training_time_vs_loss.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path= r'/home/jovyan/dlrv_project_ws/training_time_vs_loss.csv' #provide the correct filepath for the same\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Plot training loss vs time\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(data['Elapsed Time (s)'], data['Train Loss'], label='Training Loss', color='blue', marker='o')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss vs Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0422bd61-3880-4946-8667-f735437c8369",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
