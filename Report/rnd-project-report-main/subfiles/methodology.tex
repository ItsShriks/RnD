%!TEX root = ../report.tex
\documentclass[../report.tex]{subfiles}

\begin{document}
    \section{Experimental Approach}
    This section describes the experimental approach to developing and modeling a training pipeline for terrain segmentation from the 3-D point cloud LiDAR dataset. This approach consists of several steps, including data collection, pre-processing, evaluation, validation, and finally, the conclusion based on the experiments.
    
    \subsection{Tools and Environment Setup}
    The project environment was established using Anaconda to ensure reproducibility and consistency. A dedicated environment with all dependencies was maintained and shared via GitHub. Core tools included CloudCompare for 3D point cloud visualization and preprocessing, Open3D for Python-based processing, and MATLAB for supplementary analysis. SciKit-learn supported clustering and data preprocessing, while the Point Cloud Library (PCL) was employed for specialized point cloud operations during experimentation.

    \subsection{Data Pre-Processing}
    The initial dataset comprised 3D LiDAR point clouds of Field-D, captured by the Garrulus team’s UAV in a local coordinate system, without georeferencing or RGB data. It included point clouds, polygon meshes, and VTK files. A 100×100×100 m Region of Interest (ROI) was selected using CloudCompare for focused analysis, and exported as a polygon mesh for better cross-platform compatibility.

    The dataset contained several invalid points (NaN and infinite), which were retained but ignored during processing in Open3D to avoid loss of potentially useful information. Further cropping was done based on visible fencing boundaries, and the cleaned polygon mesh was finalized as a benchmark for subsequent processing and experiments.
    
    \subsection{Segmentation & Clustering Techniques}
    The Density-Based Spatial Clustering of Applications with Noise (DBSCAN)\cite{DBSCan_Grammarly} algorithm was employed to segment meaningful structures from the filtered point cloud data. DBSCAN is highly effective for clustering 3D spatial data due to its ability to detect arbitrarily shaped clusters and its resilience to noise. The input point cloud, stored in .ply format, was loaded using the Open3D library and converted into a NumPy array to facilitate clustering using the sci-kit-learn implementation of DBSCAN.
    
    The clustering process was configured with an EPS value of 0.05 and a minimum samples threshold 800. These parameters were selected based on the spatial characteristics and density of the dataset, ensuring that the algorithm could correctly differentiate between densely packed regions and outliers. Once the clustering was performed, each point was assigned a label, where points labeled -1 were considered noise, and the rest were grouped into distinct clusters.
    
    The number of clusters was determined by counting the unique labels, excluding the noise label. Color mapping was applied to visualize the clustering results, and each cluster was assigned a unique color using the jet colormap from Matplotlib. Noise points were displayed in black, enabling a clear visual distinction between separate structures in the scene. The color information was then integrated into the point cloud, and a 3D coordinate frame was added to provide spatial reference during visualization using Open3D’s interactive viewer.
    
    In addition to visual analysis, cluster-specific data could be extracted for further investigation. For example, the points associated with a particular cluster, such as the largest one (typically labeled as 0), could be isolated and analyzed independently. This approach is beneficial for identifying and studying individual objects within the environment, such as tree stumps, terrain regions, or other natural features, making DBSCAN an integral part of the segmentation pipeline.
	\subsection{Challenges Faced}
    One of the significant challenges faced during this project was the absence of labeled data in the LiDAR point cloud. Manual annotation of the point cloud data became a significantly time-consuming task without prior labeling and no direct reference to photogrammetry or RGB values. Additionally, directly labeling the point cloud would have resulted in a compromise of precision and accuracy due to the complicatedness in visually distinguishing between objects using only geometric features.

    Two auxiliary methods were introduced to address this issue. During data acquisition, the UAV system simultaneously captured both LiDAR and RGB photogrammetry data. The RGB data, being more intuitive for human interpretation, was manually annotated using QGIS. These annotations were then exported in the GeoPackage (.gpkg) format, which allowed for high-resolution spatial referencing and integration with other datasets.
    
    A secondary challenge involved the disparity in coordinate systems. The RGB photogrammetry data was geo-referenced, aligning with a global spatial reference system, whereas the LiDAR data was captured using a local coordinate system. However, due to the availability of metadata and known reference points, it was feasible to transform both datasets into a unified, standardized coordinate system. This enabled accurate overlay and cross-referencing between the manually labeled photogrammetry data and the LiDAR point cloud, ultimately improving the efficiency and reliability of the labeling process.
    
	\subsection{Data Preparation}
    Following the collection of the LiDAR datasets and their corresponding RGB photogrammetry data, along with the associated GeoPackage (.gpkg) annotations, the next critical step was extracting and aligning semantic labels. Although the GeoPackage format provided precise 2D polygonal annotations—particularly for features like tree stumps—aligning these 2D labels with the 3D LiDAR data posed a significant challenge. The process required accurately mapping flat polygonal data to volumetric point clouds, a task complicated by the lack of direct height information.

    To bridge this dimensional gap, an approximate height of 0.3 meters was assumed for stump structures based on field observations and average stump dimensions. This approximation enabled the transformation of 2D annotations into 3D labeled regions, which were then converted into .ply format to ensure compatibility with LiDAR processing tools. Subsequent alignment of the converted 3D labels with the point cloud was performed using reference-based registration and Iterative Closest Point (ICP) algorithms available in CloudCompare. This step ensured spatial accuracy between labeled regions and the actual 3D geometry of the environment.
    
    The labeled dataset was segmented into square grid tiles upon successful alignment to facilitate model training and evaluation. Each grid was assigned one of three class labels based on the contents of the points within it: 0 for terrain, 1 for stumps, and 2 for all other elements. Several permutations were tested to determine the optimal grid size. After extensive trials, a 7×7 meter grid size was selected as it provided a balanced distribution of points across categories while maintaining spatial resolution sufficient for effective classification and segmentation tasks.
    
	\subsection{Model Experiments}
    The prepared dataset, originally in CSV format, contained key geometric and structural attributes, including the x, y, and z coordinates, surface normals, curvature values, and class labels. To facilitate deep learning experiments using PointNet and PointNet++ architectures, the dataset was converted into formats compatible with each model’s input requirements.

    For the classification task using PointNet, the dataset was transformed into NumPy (.npy) arrays. The data was then organized into labeled directories corresponding to each class. A stratified split was performed to divide the dataset into training and testing subsets, allocating 70\% of the samples for training and the remaining 30\% for testing. These structured inputs were then fed into the PointNet classification model to evaluate its ability to distinguish between terrain, stumps, and other features based on point cloud geometry.
    
    In the case of PointNet++ for segmentation tasks, the dataset was formatted as 6×N matrices, where each column represented a single point consisting of x, y, and z coordinates referring to labels, curvature, and density. These were also stored in .npy format for compatibility with the model architecture. This configuration enabled the network to perform point-wise classification across the entire spatial domain, allowing fine-grained segmentation of the forest environment.
	%8.	Conclusion & Future Work
    

\end{document}
%     Describe all conceptual details about your approach in this section.
%     Add any necessary subsections to improve the presentation.

%     Feel free to rename this section to better reflect the concrete topic you are discussing.
% \end{document}
